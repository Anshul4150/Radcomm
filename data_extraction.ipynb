{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33117-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33122-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33126-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33127-i50.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33128-i50.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33216-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33220-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33256-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33310-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33326-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33328-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33402-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33434-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33501-i30.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33503-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33511-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33512-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33513-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33514-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33515-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33516-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33517-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33518-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33519-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33521-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33522-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33523-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33526-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33527-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33533-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33535-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33537-i20.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33558-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33700-28-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33737-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33738-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33739-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33740-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33741-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33809-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33848-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33858-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33874-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33875-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33876-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33877-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33882-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33883-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33884-i01.zip...\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(262144 bytes read, 752165 more expected)', IncompleteRead(262144 bytes read, 752165 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\response.py:712\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\response.py:833\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    824\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length\n\u001b[0;32m    825\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[0;32m    832\u001b[0m             \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[1;32m--> 833\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(262144 bytes read, 752165 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 934\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\response.py:905\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m<\u001b[39m amt \u001b[38;5;129;01mand\u001b[39;00m data:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001b[39;00m\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# it one byte at a time\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m     decoded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(data, decode_content, flush_decoder)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\response.py:811\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m    812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\urllib3\\response.py:729\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;66;03m# This includes IncompleteRead.\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(262144 bytes read, 752165 more expected)', IncompleteRead(262144 bytes read, 752165 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Process zip folders and extract Word documents\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mprocess_zip_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraction complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36mprocess_zip_folders\u001b[1;34m(url, output_folder)\u001b[0m\n\u001b[0;32m     24\u001b[0m zip_url \u001b[38;5;241m=\u001b[39m cells[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39ma[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading and extracting from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mdownload_and_extract_word_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36mdownload_and_extract_word_docs\u001b[1;34m(zip_url, output_folder)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract_word_docs\u001b[39m(zip_url, output_folder):\n\u001b[1;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ZipFile(BytesIO(response\u001b[38;5;241m.\u001b[39mcontent)) \u001b[38;5;28;01mas\u001b[39;00m zip_file:\n\u001b[0;32m     14\u001b[0m         zip_file\u001b[38;5;241m.\u001b[39mextractall(output_folder)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(262144 bytes read, 752165 more expected)', IncompleteRead(262144 bytes read, 752165 more expected))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL containing zip folders\n",
    "url = \"https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series\"\n",
    "\n",
    "# Function to download and extract Word documents from a zip file\n",
    "def download_and_extract_word_docs(zip_url, output_folder):\n",
    "    response = requests.get(zip_url)\n",
    "    with ZipFile(BytesIO(response.content)) as zip_file:\n",
    "        zip_file.extractall(output_folder)\n",
    "\n",
    "# Function to iterate through zip folders and extract Word documents\n",
    "def process_zip_folders(url, output_folder):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    for row in soup.find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 2 and 'href' in cells[1].a.attrs:\n",
    "            zip_url = cells[1].a['href']\n",
    "            print(f\"Downloading and extracting from {zip_url}...\")\n",
    "            download_and_extract_word_docs(zip_url, output_folder)\n",
    "\n",
    "# Output folder to store extracted Word documents\n",
    "output_folder = r\"D:\\Work\\IIT Bhilai\\Internship\\Specifications\\series_33\\Graph\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process zip folders and extract Word documents\n",
    "process_zip_folders(url, output_folder)\n",
    "\n",
    "print(\"Extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of files from Release-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33117-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33122-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33126-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33127-i50.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33128-i50.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33216-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33220-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33256-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33310-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33326-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33328-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33402-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33434-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33501-i30.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33503-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33511-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33512-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33513-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33514-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33515-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33516-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33517-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33518-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33519-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33521-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33522-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33523-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33526-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33527-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33533-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33535-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33537-i20.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33558-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33700-28-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33737-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33738-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33739-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33740-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33741-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33809-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33848-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33858-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33874-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33875-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33876-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33877-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33882-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33883-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33884-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33886-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33887-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33890-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33891-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33892-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33893-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33894-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33896-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33898-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33916-i00.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33926-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33927-i01.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33928-i10.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series/33936-i01.zip...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL containing zip folders\n",
    "url = \"https://www.3gpp.org/ftp/Specs/2023-09/Rel-18/33_series\"\n",
    "\n",
    "# Function to download and extract Word documents from a zip file\n",
    "def download_and_extract_word_docs(zip_url, output_folder):\n",
    "    response = requests.get(zip_url)\n",
    "    with ZipFile(BytesIO(response.content)) as zip_file:\n",
    "        zip_file.extractall(output_folder)\n",
    "\n",
    "# Function to iterate through zip folders and extract Word documents\n",
    "def process_zip_folders(url, output_folder, processed_urls=[]):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    for row in soup.find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 2 and 'href' in cells[1].a.attrs:\n",
    "            zip_url = cells[1].a['href']\n",
    "            if zip_url not in processed_urls:\n",
    "                print(f\"Downloading and extracting from {zip_url}...\")\n",
    "                try:\n",
    "                    download_and_extract_word_docs(zip_url, output_folder)\n",
    "                    processed_urls.append(zip_url)\n",
    "                    # Optionally, store processed URLs in a file\n",
    "                    with open('processed_urls.txt', 'a') as f:\n",
    "                        f.write(zip_url + '\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {zip_url}: {e}\")\n",
    "                    # Log the error or handle it as needed\n",
    "\n",
    "# Output folder to store extracted Word documents\n",
    "output_folder = r\"D:\\Work\\IIT Bhilai\\Internship\\Specifications\\series_33\\Graph\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List to store processed URLs\n",
    "processed_urls = []\n",
    "\n",
    "# Check if there is a file with processed URLs and load it\n",
    "if os.path.exists('processed_urls.txt'):\n",
    "    with open('processed_urls.txt', 'r') as f:\n",
    "        processed_urls = [line.strip() for line in f]\n",
    "\n",
    "# Process zip folders and extract Word documents\n",
    "try:\n",
    "    process_zip_folders(url, output_folder, processed_urls)\n",
    "    print(\"Extraction complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Remove the 'processed_urls.txt' file if all URLs are successfully processed\n",
    "if len(processed_urls) > 0 and len(processed_urls) == len(set(processed_urls)):\n",
    "    os.remove('processed_urls.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only the files which are latest. Keep in mind start from the most latest release and go back to other releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33102-390.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33103-360.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33105-380.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33106-310.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33107-320.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33120-300.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33901-300.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33902-310.zip...\n",
      "Downloading and extracting from https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series/33908-300.zip...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL containing zip folders\n",
    "url = \"https://www.3gpp.org/ftp/Specs/2001-06/R1999/33_series\"\n",
    "\n",
    "# Function to download and extract Word documents from a zip file\n",
    "def download_and_extract_word_docs(zip_url, output_folder):\n",
    "    response = requests.get(zip_url)\n",
    "    with ZipFile(BytesIO(response.content)) as zip_file:\n",
    "        # Extract each file individually\n",
    "        for file_info in zip_file.infolist():\n",
    "            # Check if the first 5 letters of the file name exist in the output folder\n",
    "            if not any(file_info.filename[:5] in existing_file for existing_file in os.listdir(output_folder)):\n",
    "                zip_file.extract(file_info, output_folder)\n",
    "\n",
    "# Function to iterate through zip folders and extract Word documents\n",
    "def process_zip_folders(url, output_folder, processed_urls=[]):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    for row in soup.find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) >= 2 and 'href' in cells[1].a.attrs:\n",
    "            zip_url = cells[1].a['href']\n",
    "            if zip_url not in processed_urls:\n",
    "                print(f\"Downloading and extracting from {zip_url}...\")\n",
    "                try:\n",
    "                    download_and_extract_word_docs(zip_url, output_folder)\n",
    "                    processed_urls.append(zip_url)\n",
    "                    # Optionally, store processed URLs in a file\n",
    "                    with open('processed_urls.txt', 'a') as f:\n",
    "                        f.write(zip_url + '\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {zip_url}: {e}\")\n",
    "                    # Log the error or handle it as needed\n",
    "\n",
    "# Output folder to store extracted Word documents\n",
    "output_folder = r\"D:\\Work\\IIT Bhilai\\Internship\\Specifications\\series_33\\Graph\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List to store processed URLs\n",
    "processed_urls = []\n",
    "\n",
    "# Check if there is a file with processed URLs and load it\n",
    "if os.path.exists('processed_urls.txt'):\n",
    "    with open('processed_urls.txt', 'r') as f:\n",
    "        processed_urls = [line.strip() for line in f]\n",
    "\n",
    "# Process zip folders and extract Word documents\n",
    "try:\n",
    "    process_zip_folders(url, output_folder, processed_urls)\n",
    "    print(\"Extraction complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Remove the 'processed_urls.txt' file if all URLs are successfully processed\n",
    "if len(processed_urls) > 0 and len(processed_urls) == len(set(processed_urls)):\n",
    "    os.remove('processed_urls.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
