# -*- coding: utf-8 -*-
"""RAG_Pinecone_Multiple_Docs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18N60-9tMiksXWvKlleI-WEQQ_KFU64Se
"""

pip install -qU datasets==2.12.0 apache_beam mwparserfromhell

def read_word_document(txt_file_name):
    with open(txt_file_name, 'r') as file:
      document_text = file.read()
    return document_text

# Use the name of the uploaded file in place of 'your_uploaded_file.docx'
uploaded_file_path = '/content/drive/MyDrive/Spec_Parsed_File/51026-h00.zip_parsed.txt'
document_text = read_word_document(uploaded_file_path)

print(document_text[:500])  # Print the first 500 characters as a preview

!pip install -qU langchain==0.0.162 openai==0.27.7 tiktoken==0.4.0 "pinecone-client[grpc]"==2.2.2

import tiktoken

tiktoken.encoding_for_model('gpt-3.5-turbo')

import tiktoken

tokenizer = tiktoken.get_encoding('cl100k_base')

# create the length function
def tiktoken_len(text):
    tokens = tokenizer.encode(
        text,
        disallowed_special=()
    )
    return len(tokens)

tiktoken_len("hello I am a chunk of text and using the tiktoken_len function "
             "we can find the length of this chunk of text in tokens")

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=10,
    length_function=tiktoken_len,
    separators=["\n\n", "\n", " ", ""]
)

chunks = text_splitter.split_text(document_text)
chunks

#tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])

import os

# get openai api key from platform.openai.com
OPENAI_API_KEY = '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'
OPENAI_API_KEY

from langchain.embeddings.openai import OpenAIEmbeddings

model_name = 'text-embedding-ada-002'

embed = OpenAIEmbeddings(
    model=model_name,
    openai_api_key=OPENAI_API_KEY
)

texts = [
    'this is the first chunk of text',
    'then another second chunk of text is here'
]

res = embed.embed_documents(texts)
len(res), len(res[1])

index_name = 'langchain-rag-multiple-docs'

import pinecone

# find API key in console at app.pinecone.io
PINECONE_API_KEY = '4496f4e4-177a-4733-b74f-fd3fea167a15'
# find ENV (cloud region) next to API key in console
PINECONE_ENVIRONMENT =  'gcp-starter'

pinecone.init(
    api_key=PINECONE_API_KEY,
    environment=PINECONE_ENVIRONMENT
)

if index_name not in pinecone.list_indexes():
    # we create a new index
    pinecone.create_index(
        name=index_name,
        metric='cosine',
        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002
    )

index = pinecone.GRPCIndex(index_name)

index.describe_index_stats()

data = document_text
data

import os
from tqdm.auto import tqdm
from uuid import uuid4
import time

# Assuming your text_splitter, embed, and other relevant functions are defined

# Folder path containing multiple text files
folder_path = '/content/drive/MyDrive/Spec_Parsed_File'

batch_limit = 50
texts = []
metadatas = []

# Function to process a batch of texts
def process_batch(batch_texts, batch_metadatas):
    ids = [str(uuid4()) for _ in range(len(batch_texts))]
    embeds = embed.embed_documents(batch_texts)
    time.sleep(0.25)
    index.upsert(vectors=zip(ids, embeds, batch_metadatas))

# Iterate over files in the folder
for filename in tqdm(os.listdir(folder_path)):
    # Add a condition to process only .txt files
    if filename.endswith('.txt'):
        file_path = os.path.join(folder_path, filename)
        print("File_Path:", file_path)
        data = read_word_document(file_path)
        # Metadata for each file
        metadata = {
            'file_name': filename  # Add the filename to metadata
        }
        # Now create chunks from the record text
        record_texts = text_splitter.split_text(data)
        print("Length of record texts:", record_texts)
        # Create individual metadata dicts for each chunk
        record_metadatas = [{
            "chunk": j, "text": text, **metadata
        } for j, text in enumerate(record_texts)]
        # Append non-empty record_texts to current batches
        texts.extend(text for text in record_texts if len(text.strip()) > 0)
        metadatas.extend(record_metadatas)
        # If we have reached the batch_limit, process the batch
        while len(texts) >= batch_limit:
            print(len(texts))
            # Take a batch of texts and metadatas
            batch_texts = texts[:batch_limit]
            batch_metadatas = metadatas[:batch_limit]
            # Remove the processed batch from the original lists
            texts = texts[batch_limit:]
            metadatas = metadatas[batch_limit:]
            # Process the batch
            process_batch(batch_texts, batch_metadatas)

# Process any remaining texts
if len(texts) > 0:
    process_batch(texts, metadatas)

index.describe_index_stats()

from langchain.vectorstores import Pinecone

text_field = "text"

# switch back to normal index for langchain
index = pinecone.Index(index_name)

vectorstore = Pinecone(
    index, embed.embed_query, text_field
)

query = "What is frequency hopping"

vectorstore.similarity_search(
    query,  # our search query
    k=3  # return 3 most relevant docs
)

from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# completion llm
llm = ChatOpenAI(
    openai_api_key=OPENAI_API_KEY,
    model_name='gpt-3.5-turbo',
    temperature=0.0
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

qa.run(query)